{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic search index creation \n",
    "\n",
    "Anton Antonov  \n",
    "April 2025  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to create an LLM-computed vector database over the paragraphs of relatively large text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Retrieval Augmented Generation (RAG) workflow we consider:\n",
    "\n",
    "- The document collection is ingested.\n",
    "- The documents are split into chunks of relevant sizes.\n",
    "- Large Language Model (LLM) embedding vectors are obtained for all chunks.\n",
    "- A vector database is created with these embedding vectors and stored locally. Multiple local databases can be created.\n",
    "- A relevant local database is imported for use.\n",
    "- An input query is provided to a retrieval system.\n",
    "- The retrieval system retrieves relevant documents based on the query.\n",
    "- The top K documents are selected for further processing.\n",
    "- The model is fine-tuned using the selected documents.\n",
    "- The fine-tuned model generates an answer based on the query.\n",
    "- The output answer is presented to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a Mermaid-JS component diagram that shows the components of performing the Retrieval Augmented Generation (RAG) workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use Data::Importers;\n",
    "use LLM::Functions;\n",
    "use XDG::BaseDirectory :terms;\n",
    "\n",
    "use LLM::RetrievalAugmentedGeneration;\n",
    "use LLM::RetrievalAugmentedGeneration::VectorDatabase;\n",
    "\n",
    "use Data::Reshapers;\n",
    "use Data::Summarizers;\n",
    "use JSON::Fast;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my %h = num-type => num32.^name;\n",
    "\n",
    "to-json(%h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Ingest text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training texts file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my $dirName = ($*CWD ~ '/texts/DialogueWorks/Wolff-Hudson').subst('/notebooks/Jupyter');\n",
    "my @fileNames = dir($dirName).grep(*.ends-with('.txt'));\n",
    "\n",
    "$dirName = ($*CWD ~ '/texts/RobinsonErhardt/Wolff-Hudson').subst('/notebooks/Jupyter');\n",
    "@fileNames .= append(dir($dirName).grep(*.ends-with('.txt')));\n",
    "\n",
    "@fileNames.elems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my %texts = @fileNames.map({ $_.basename => slurp($_)});\n",
    "deduce-type(%texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show text sizes summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink records-summary(%texts.values.deepmap(*.&text-stats)Â».Hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a sample of the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% html\n",
    "%texts.head(4)\n",
    "==> { .Hash.deepmap(*.substr(0..120)) }()\n",
    "==> to-html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Make vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** The vector database can be made by just specifying the directory with text files. Here we use \"low level\" approach in order to experiment with different text modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an empty vector database object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my $vdbObj = LLM::RetrievalAugmentedGeneration::VectorDatabase.new(name => 'EconomicsAI');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an LLM access specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my $conf = llm-configuration(\"ChatGPT\", model => 'text-embedding-002');\n",
    "my $conf = llm-configuration(\"Gemini\");\n",
    "#my $conf = llm-configuration('LLaMA', model => 'llama-embedding');\n",
    "\n",
    "$conf.Hash.elems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the semantic index for the vector database object (an profile it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my $tstart = now;\n",
    "$vdbObj.create-semantic-search-index(%texts, method => 'by-max-tokens', max-tokens => 1024, e => $conf):embed;\n",
    "my $tend = now;\n",
    "say \"Time to make the semantic search index: {$tend - $tstart} seconds.\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the vector database object is exported in a sub-directory of [`$XDG_DATA_HOME`](https://specifications.freedesktop.org/basedir-spec/latest/index.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sub-directory\n",
    "my $dirname = data-home.Str ~ '/raku/LLM/SemanticSearchIndex';\n",
    "\n",
    "# The exported vector database base file name\n",
    "my $basename = \"SemSe-{$vdbObj.id}.json\";\n",
    "\n",
    "# Corresponding IO:Path object\n",
    "my $file = IO::Path.new(:$dirname, :$basename);\n",
    "\n",
    "# Check for existence\n",
    "$file.f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The export path is saved in the vector database object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$file.Str eq $vdbObj.location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a sample of the text chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% html\n",
    "$vdbObj.items.pairs.pick(4).sort(*.key) ==> to-html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show dimensions and data type of the obtained vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "say \"dimensions : \", $vdbObj.vectors.&dimensions;\n",
    "say \"data type  : \", deduce-type($vdbObj.vectors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skim vector databases from the default directory and show summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% html\n",
    "my @field-names = <id name item-count dimension version llm-service llm-embedding-model created>;\n",
    "vector-database-objects(f=>'hash', :flat)\n",
    "==> { $_.map({ $_<created> = $_<file>.IO.created.DateTime.Str.subst('T',' ').substr(^19); $_}).sort(*<created>).reverse }()\n",
    "==> to-html(:@field-names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles\n",
    "\n",
    "[AA1] Anton Antonov, \n",
    "[\"Outlier detection in a list of numbers\"](https://rakuforprediction.wordpress.com/2022/05/29/outlier-detection-in-a-list-of-numbers/),\n",
    "(2022),\n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "[AAp1] Anton Antonov,\n",
    "[WWW::OpenAI Raku package](https://github.com/antononcube/Raku-WWW-OpenAI),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp2] Anton Antonov,\n",
    "[WWW::PaLM Raku package](https://github.com/antononcube/Raku-WWW-PaLM),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp3] Anton Antonov,\n",
    "[LLM::Functions Raku package](https://github.com/antononcube/Raku-LLM-Functions),\n",
    "(2023-2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp4] Anton Antonov,\n",
    "[LLM::Prompts Raku package](https://github.com/antononcube/Raku-LLM-Prompts),\n",
    "(2023-2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp5] Anton Antonov,\n",
    "[ML::FindTextualAnswer Raku package](https://github.com/antononcube/Raku-ML-FindTextualAnswer),\n",
    "(2023-2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp6] Anton Antonov,\n",
    "[Math::Nearest Raku package](https://github.com/antononcube/Raku-Math-Nearest),\n",
    "(2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp7] Anton Antonov,\n",
    "[Math::DistanceFunctions Raku package](https://github.com/antononcube/Raku-Math-DistanceFunctions),\n",
    "(2024),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp8] Anton Antonov,\n",
    "[Statistics::OutlierIdentifiers Raku package](https://github.com/antononcube/Raku-Statistics-OutlierIdentifiers),\n",
    "(2022),\n",
    "[GitHub/antononcube](https://github.com/antononcube)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos\n",
    "\n",
    "[CWv1] Chris Williamson,\n",
    "[\"Eric Weinstein - Why Does The Modern World Make No Sense? (4K)\"](https://www.youtube.com/watch?v=p_swB_KS8Hw),\n",
    "(2024),\n",
    "[YouTube/@ChrisWillx](https://www.youtube.com/@ChrisWillx).   \n",
    "([transcript](https://podscripts.co/podcasts/modern-wisdom/747-eric-weinstein-why-does-the-modern-world-make-no-sense).)\n",
    "\n",
    "[CWv2] Chris Williamson,\n",
    "[\"Eric Weinstein - Are We On The Brink Of A Revolution? (4K)\"](https://www.youtube.com/watch?v=PYRYXhU4kxM),\n",
    "(2024),\n",
    "[YouTube/@ChrisWillx](https://www.youtube.com/@ChrisWillx).   \n",
    "([transcript](https://podscripts.co/podcasts/modern-wisdom/833-eric-weinstein-are-we-on-the-brink-of-a-revolution).)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RakuChatbook",
   "language": "raku",
   "name": "raku"
  },
  "language_info": {
   "file_extension": ".raku",
   "mimetype": "text/x-raku",
   "name": "raku",
   "version": "6.d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
